# Simulation of CROSP SoC

This directory includes code for Verilator simulation.

## Hierarchy

The source files are listed below:

- `vcore.sv`: Additional wrapper of core, for extraction of
  verification states by HDL code. Currently the only
  implementation of CROSP core is `crospaxi`.
- `soc.cpp`: together with `soc.h` includes the definition of
  several SoC components in HDL, including verification core,
  interrupt controller, and peripheral drivers.
- `verif.cpp`: Together with `verif.h`, includes abstract AXI
  device class and definition of abstract physical memory, to
  handle memory R/W and other abstract memory-mapped operations
  such as HTIF protocol. There is also definition of machine
  state and delta for verification, together with some utility
  functions to calculate and log state transitions.
- `main.cpp`: It includes the main function to handle command
  line, instantiate and initialize all components, run
  simulation and handle interconnects. During simulation, an
  optional checker is provided for verification.
- `sim.cpp`: This source file can build a simulator using
  verification tools in `verif.cpp` to run program in pure C++
  independently without HDL simulation. It can be seen as the
  reference model used in core verification.

## Build

The simulation uses Verilator, and for testing, RISC-V
toolchain is also recommended to be installed. A pre-built
docker image can be found at
[dockerhub](https://hub.docker.com/r/microzero/riscv-sim). Run
```
docker run -it -v .:/work microzero/riscv-sim:verilator
```
to create and run a container. After toolset is properly
installed, run
```
make -j
```
to build executable `main` to simulate with Verilator. Run
`./main -h` to show details of command line.

Some variables are supported in makefile to determine basic
parameters and pre-define some simulation behavior. Firstly the
parameters of SystemVerilog modules are defined in makefile
which can be changed directly or overwritten in command line.

## Verification flow

There are four steps in current verification flow.

1. Hex code test, run `make hex` to verify all hex codes.
2. RV standard test, run `make rv` to verify all rv codes.
3. Proxy kernel test, run `make pk PKARG="..."` to run PK.
4. Linux test.

The above procedure is related to three types of initializing
memory.

1. HEX mode: Read from text file including hex codes.
2. ELF mode: Read from ELF (executable linkable file).
3. BIN mode: Read from binary file.

HEX mode is designed to verify some basic correctness like
connection and data path, it can be generated by assembler or
compiler from user-defined code. For example, the assembler
[RARS](https://github.com/TheThirdOne/rars.git) is a Java
program which can easily dump hex code of assembly.

ELF mode is mainly to adapt standard programs and verification
tools, such as
[RISC-V tests](https://github.com/riscv-software-src/riscv-tests)
and other programs with standard format like proxy kernel and
OpenSBI programs.

BIN mode is mainly to verify programs related to synthesis, as
it is simple enough to load into memory. Because of the
specialness of programs, different programs have different
loading method. This will be identified by filename. For
example, Buildroot will load first 32M in binary file to initrd
address, while load second 32M to code address.

Additionally, to support PK and OS, two special memory loading
are supported:

1. DTB: Load device tree blob to address specified in makefile.
2. INITRD: Load initrd file to address specified in makefile.

Here are some examples for different test steps.

### Hex code test

Run `make hex` first, if error detected in a program,
`../util/code/hex/0.hex` for instance, we can run into this
specific program and debug. Run
```
./main -s -v -w -hex ../util/code/hex/0.hex
```
and the flag `-v` will log every committed instructions before
the error being detected. `-w` is to output waveform with names
`clint.vcd`, `vcore.vcd`, etc.

### RV test

Run `make rv` first, and next step is similar with hex code.
For example, benchmark test `dhrystone` encounters error and
exit at cycle `200000`, and it may be inefficient to record
waveform for such long time. So run
```
./main -s -v -w -t 199000 -1 \
    $RISCV/target/share/riscv-tests/benchmarks/dhrystone.riscv
```
and the simulation will start log and record waveform from
cycle `199000` to `-1` (unsigned integer maximum).

### PK test

Proxy kernel is part of RISC-V test suite which can be found in
[github](https://github.com/riscv-software-src/riscv-pk). It is
an application execution environment which can host
statically-linked ELF files and proxy system some calls with
support of host machine. After PK is installed at default
position, we can use
```
make -C ../util/code/user/
make pk PKARG="../util/code/user/arg hello world"
```
for example to run code with arguments specified in `PKARG` in
user mode and virtual address space.

### Buildroot test

Buildroot is previously supported for running on FPGA with HTIF
protocol filter. As Debian is now supported, the HTIF-UART
filter is now removed, but Buildroot can still be tested in
simulation environment. Buildroot images include kernel, device
tree and initial RAM image. The kernel can be packed in OpenSBI
firmware payload, or use independent kernel file. The
simulation environment now supports firmware payload format,
and then all the kernel code with SBI firmware will packed into
a single ELF file.

After Buildroot is build in official repository, the typical
images include kernel `fw_payload.elf`, initial RAM
`initrd.cpio`. Then run
```
make -C ../util/dt
./main -s -dtb ../util/dt/spike.dtb \
  -initrd "path-to-initrd" "path-to-kernel"
```

### Debian test

The Debian system in repository `vivado-risc-v` introduces
peripherals with AXI interface, including SD card controller,
UART controller, and Ethernet controller. These controllers in
Verilog HDL are packed with C++ classes and are simulated with
CROSP core. The repository also provides a ROM image
`bootrom.elf` for bootloading. We will use this ELF file as
initial code to run.

The bootloader will access SD card controller using AXI
interconnection, so that the C++ class of SD controller will
access SD image file and emulate SD physical layer to assign
IO of SD controller DUT. The SD image file can also be found in
`vivado-risc-v` repository, and should be renamed to
`sdc.img` in `sim` directory.

The device tree is assembled in BOOTROM, and the addresses of
devices can be found in this device tree. So after SD card
image and BOOTROM is properly configured, simply run
```
./main -s "path-to-bootrom"
```
and start a simulation of Debian system.

### Checkpoint support

Sometimes running a simulation like OS is too long, about tens
of hours. Checkpoint is a useful technique to raise debugging
efficiency in long simulation. The simulation program `./main`
records checkpoint files like `amem.save` in `sim` directory
per 10M cycle. It is overwritten to save disk space. We can use
```
make tar PREFIX="file-prefix"
```
to archive a checkpoint, and recover using
```
tar xf 300M.tar.gz
```
if `file-prefix` is `300M`.

After checkpoint files is properly set in `sim` directory, if
no input file is specified in command line, the `main` program
will by default load from checkpoint. Simply run
```
./main -s
```
and the simulation will start from checkpoint.
